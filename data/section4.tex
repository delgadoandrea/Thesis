%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  New template code for TAMU Theses and Dissertations starting Fall 2016.  
%
%
%  Author: Sean Zachary Roberson
%  Version 3.17.09
%  Last Updated: 9/21/2017
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                           SECTION IV
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\chapter{EVENT RECONSTRUCTION \label{cha:eventreco}}
During proton-proton collisions, interactions happen in a small longitudinal region near the center called the luminous region or interaction region. The origin of one or more new particles is called a vertex. In this analysis, the set of particle measurements in the CMS detector associated to a single beam-beam crossing will be referred to as an event. This section explains how a reconstruction software is used to process the raw information and identify physics objects for a given event. 

 \begin{figure}[H]
 	\centering
 	\includegraphics[width=0.75\textwidth]{figures/eventvertex.png}
 	\singlespace
 	\caption{Schematic diagram for a reconstructed event at the LHC.}
 	\label{fig:vertex}
 \end{figure}


\section{Data Acquisition}
The beams circulating the LHC ring during its data-taking period are comprised of bunches of protons with a design spacing of 25 ns, which corresponds to a collision rate of 40 MHz. At this rate, it would be impossible to keep or even transmit all the collected data.

The CMS Data Acquisition (DAQ) and trigger system was specifically designed to cope with the large amount of data acquired by the detector. Only a small fraction of events are actually written to disk and an event filtering is performed online by the so-called trigger systems.

 The interaction point of a particular proton-proton collision is referred to as the primary vertex (PV), all other proton-proton interactions that took place during the same bunch-crossing are called pile-up. Secondary vertices refer to those production points (other than the PV) where particles are created either from the decay or hard-scattering of the particles associated with a particular PV. This is shown in Figure \ref{fig:vertex}. The higher the collision energy, the more interesting the collision, since higher energy collisions are more likely to produce particles or interactions that have not yet been observed. For this reason, triggers are configured to prefer higher momentum objects. 

\subsection{L1 Trigger and HLT}
 Whenever the LHC is performing at its peak, about one billion proton-proton interactions take place every second inside the CMS detector. To select events of potential physics interest, the CMS trigger\cite{Khachatryan_2017} utilizes a two-level system including an L1 hardware trigger and an HLT array of commercially available computers running high-level physics algorithms.

The L1 of the CMS trigger is an extremely fast process that selects events containing candidate objects, e.g. ionization deposits consistent with a muon, or energy clusters consistent with an electron, photon, $\tau$ lepton, missing transverse energy ($E_{T}^{miss}$, defined as the modulus of the verctor sum of the $p_{T}$ of all reconstructed particles), or jet. During this process, only coarsely segmented data from calorimeter and muon detectors is used, while all the high-resolution data is held in pipeline memories in the front-end electronics. At the end of this process, no more than 100 kHz of the stored events are forwarded to the next trigger system, the HLT.  

 \begin{figure}[h]
 	\centering
 	\includegraphics[width=0.7\textwidth]{figures/The-CMS-Level-1-Trigger.png}
 	\singlespace
 	\caption{The CMS Level-1 Trigger. Reprinted from \cite{cmsglobaltrigger}}
 	\label{fig:l1trig}
 \end{figure}

The data processing of the HLT is structured around the concept of an \textit{HLT path}, which is a set of algorithmic processing steps run in a predefined order that rudimentarily reconstruct physics objects based on these HLT objects. The reconstruction modules and selection filters of the HLT use the software framework that is also used for offline reconstruction and analyses.

As a result of this procedure, the average rate of accepted events is reduced to about 400 Hz. Event data is stored locally on disk and eventually transferred to the CMS Tier-0 computing center for offline processing and permanent storage.


\subsection{T1 sites and data storage}

CMS computing operates on a tiered computing structure. A Tier-0 computing center is located at CERN where the data is transferred to the HLT and a first set of reconstruction occurs. From there, it is transferred to one of seven Tier-1 computing centers located around the world. At the Tier-1 centers, a full reconstruction of the data is performed. Furthermore, there are 55 Tier-2 centers which can be accessed by the collaboration members for data processing and storage.

The analysis presented here was performed at one of the Tier-3 centers, the Texas A$\&$M University Brazos HPC cluster\cite{brazosc}. 

 \begin{figure}[H]
 	\centering
 	\includegraphics[width=0.75\textwidth]{figures/dataflowtiers_MC.png}
 	\singlespace
 	\caption{Flow of CMS detector data through the tiers. Reprinted from \cite{CMSdatatier}}
 	\label{fig:datatier}
 \end{figure}

 The data itself is also processed in three data tiers. The first layer of this is the RAW data, which is created by unpacking detector streams passed on from the HLT trigger. It is composed of measurements from the different sub-detectors, as well as some information provided by the HLT and L1 triggers. RAW data is then reconstructed into PF objects, as explained in Section \ref{sec:track}. This step is called RECO, which is short for reconstruction and contains both the detector and physics object information. 

 After the RECO step, \textit{analysis object data} (AOD) is generated from a subset of the RECO information. AOD objects are typically comprised of only high-level physics objects, and therefore, of reduced size. 

\section{Particle Flow Event Reconstruction}
\label{sec:track}

 In the previous section we described how data was managed and stored during the acquisition process. This section will focus on how raw detector information is interpreted.  

Raw detector data is measured in the form of hits in the tracker or the muon system, as well as energy depositions in the calorimeters. Then, the trajectories of charged particles, or tracks, are reconstructed from the position hits in the tracker. From the collection of tracks in an event, interaction vertices are reconstructed. 

An optimal event description can be achieved by correlating the basic elements from all subdetectors (tracks and clusters) to identify each final-state particle, and by combining the corresponding measurements to reconstruct the particle properties on the basis of this identification. At CMS, this approached is called \textit{particle-flow (PF) reconstruction}.

The reconstructed and identified individual particle list includes muons, electrons, photons, as well as charged  and neutral hadrons. This list of individual particles is then used to build jets (from which the quark and gluon four-momentum in inferred), to determine the $E_{T}^{miss}$ (which gives an estimate of the direction and energy of the neutrinos and other invisible particles), to reconstruct and identify taus from their decay products, etc.

 \begin{figure}[h]
 	\centering
 	\includegraphics[width=0.75\textwidth]{figures/image005.png}
 	\singlespace
 	\caption{Cross-sectional view of the CMS detector with all of the sub-detectors labeled. The colored lines correspond to different particle types. Each particle interacts with different pieces of the detector and may or may not be bent by the magnetic field. Reprinted from \cite{CMSSlice}}
  	\label{fig:cmsslice} 	
 \end{figure}

During PF reconstruction, photons and neutral hadrons are identified by ECAL and HCAL clusters with no associated tracks, respectively. Electrons can be identified by associating a track to an ECAL cluster and from possible Bremsstrahlung photons radiated by the electron in the tracker on its way to the ECAL; with a momentum-to-energy ratio compatible with unity, and not connected to an HCAL cluster. Finally, muons and neutrinos would traverse the calorimeters with little or no interactions. While neutrinos would escape undetected, muons would be identified by a track in the inner tracker connected to a track in the muon detectors. Muons are reconstructed in isolation as well as in jets. Finally, the presence of neutrinos can be detected by the $E_{T}^{miss}$ in the event. See Figure \ref{fig:cmsslice}.

The PF concept was developed and used for the first time by the ALEPH experiment at LEP\cite{BUSKULIC1995481}. In particular, CMS is very well suited for PF reconstruction due to its highly-segmented tracker,  a fine-grained ECAL, and an hermetic HCAL.

Also, the CMS magnet is large enough to accommodate the tracker and both the ECAL and HCAL, thereby minimizing the amount of material in front of the calorimeters.

% This particular feature is an advantage for PF reconstruction, as it eliminates the potential energy loss before the calorimeters caused by particles showering in the coil material.

In the following sections, the general PF reconstruction work-flow will be described, starting by the reconstruction of its fundamental elements, the charged particle tracks and the calorimeter clusters. These elements are then grouped and interpreted in terms of particles.

\subsection{Iterative Tracking}

The first step of the PF reconstruction process consists of the reconstruction of hits in the pixel and strip tracker\cite{TRK-11-001} and is referred to as local reconstruction.

The next step is track reconstruction, which refers to the process of using the reconstructed hits to obtain estimates for the momentum and position parameters of the charged particles responsible for the tracker hits. The momentum of charged hadrons is measured in the tracker with a resolution vastly superior to that in the calorimeter. Furthermore, the tracker provides a precise measurement of the charged particle direction at the production vertex. 

The tracking software at CMS\cite{TRK-11-001} is commonly referred to as the combinatorial Track Finder (CTF), which is an adaptation of the combinatorial Kalman Filter \cite{Billoir:1989mh,BILLOIR1990219,Mankel:1997dy}, which in turn is an extension of the Kalman filter\cite{Fruhwirth:1987fm} to allow pattern recognition and track fitting to occur in the same framework. The collection of reconstructed tracks is produced by multiple passes or iterations of the same CTF track reconstruction sequence, in a process called iterative tracking. 

The basic idea of iterative tracking is that tracks of relatively large $p_{T}$ and those produced near the interaction region are searched for during the initial iteration. During successive iterations, hits unambiguously assigned to candidates found in previous iterations tracks are removed. By doing so, the combinatorial complexity is reduced, and subsequent iterations searching for more difficult types of tracks (e.g., low $p_{T}$, or greatly displaced tracks) is simplified.

Each iteration proceeds in four steps:

\begin{itemize}
	\item Seed generation, which provides track candidates consisting of a few (2 or 3) hits. Seeds are generated in the innermost layers of the tracker and are commonly referred to as \textit{proto-tracks}.
	\item Track finding, which is based on a Kalman filter. It extrapolates the seed trajectories along the expected flight path of a charged particle, searching for additional hits that can be assigned to the track candidate.
	\item Track fitting. A module that is used to provide the best possible estimate of the parameters of each trajectory by means of a Kalman filter.
	\item Track selection. This step sets the quality flags and discards tracks that fail certain specified criteria.
\end{itemize}

A total of six iterations are used, each with different seed generation, $p_{T}$, and impact parameter requirements. The first iterations follow multiple criteria in order to achieve a negligible small fake rate. Once the hits that are associated with so-called \textit{fake tracks} are removed, the seeding criteria is loosened, and therefore, tracking efficiency is increased. From iteration 4 and on, the constraints on the tracks closer to the interaction point are slowly relaxed. This allows for reconstruction of secondary charged particles created from photon conversions and nuclear interactions in the tracker volume.

\subsection{Calorimeter Clustering}
Clustering in the calorimeters is the process of grouping detector cells that register hits together with the purpose of (i) detecting and measuring the energy and direction of stable neutral particles, (ii) being able to separate these neutral particles from energy deposits associated with charged hadrons, (iii) reconstructing and identifying electrons and all possible Bremsstrahlung photons, and (iv) helping the energy measurement of charged hadrons for which the track parameters were not determined accurately, which is sometimes the case for high-$p_{T}$ tracks.

The clustering algorithm is performed separately in each of the following sub-detectors: ECAL barrel and endcap, HCAL barrel and endcap, as well as in the pre-shower. It proceeds via three steps\cite{CMS:2009nxa}:

\begin{enumerate}
	\item Identify 'cluster seeds'. These are defined as the cell in a calorimeter with a local maximum of energy (above some set threshold).
	\item Expand from the seed to grow 'topological clusters'. This is done by aggregating calorimeter cells that have at least one side in common with the seed cell, and also have an energy over a particular threshold.
	\item Repeat the process of cluster growing, now using new cells that are part of the cluster.
\end{enumerate}

In this sense, a "seed" gives rise to a "particle-flow cluster". If a cell is identified by two clusters, the energy is shared between the clusters according to the distance from the cell to the center of each cluster. The cluster energies and positions are iteratively determined as new cells are added to the cluster.

\subsection{Linking Tracks and Clusters}
Once the basic PF elements are available, the next step in reconstructing a particle is the so-called \textit{link algorithm}. This algorithm can test any pair of elements in the event. In order to prevent the computing time of the link algorithm from growing quadratically with the number of particles, the pairs of elements considered by the link procedure are restricted to the nearest neighbors in the ($\eta,\phi$) plane, as obtained with a $k$-dimensional tree\cite{Bentley1975MultidimensionalBS}. 

 If two elements are found to be linked, the algorithm defines a metric between these two elements, aimed at quantifying the quality of the link. The link algorithm then produces \textit{PF blocks} of elements associated either by a direct link or by an indirect link through common elements.

 The link between tracks and calorimeter clusters proceeds by extrapolating the last measured hit in the tracker to one of the three detectors\cite{CMS:2009nxa}:

 \begin{itemize}
 	\item The two layers of the pre-shower detector,
 	\item the ECAL, at a depth corresponding to the expected maximum of the electron shower profile,
 	\item the HCAL, to a depth corresponding to one interaction length.
 \end{itemize}

 The track is then linked to a cluster in these detectors if the extrapolated position is within the cluster boundaries. Additionally, to link Bremsstrahlung photons to their associated electron, tangents to the track are extrapolated to the ECAL and any cluster found within those boundaries is also linked.

 Similarly, links between the calorimeters are formed when a cluster from the more granular calorimeter (pre-shower or ECAL) is within the cluster envelope of the less granular calorimeter (ECAL or HCAL).

 Finally, muon tracks are linked to charged particle tracks by a global fit between the two sets of tracks.

 \section{Physics Object Reconstruction}

  \begin{figure}[h]
 	\centering
 	\includegraphics[width=0.75\textwidth]{figures/jets.png}
 	\singlespace
 	\caption{CMS Particle Flow algorithm. The diagram shows how collisions lead to particle decays and final state particles. On the right side of the diagram the tracks and deposits in the CMS detector are shown. The left side shows that PF candidates are derived from detector information and then become input for the PF algorithm that uses them to construct high-level physics objects like electrons, which are then used by analysts to reconstruct the collision event. Reprinted from \cite{CMS-PAS-PFT-09-001}}
  	\label{fig:pf} 	
 \end{figure}

With the tracks identified, calorimeter clusters formed and the linking of clusters to tracks, particles can then be reconstructed. The PF process begins by reconstructing muons, then electrons and photons, and finally charged hadrons. As each particle is reconstructed, the tracks and clusters associated with it are removed from the collection of blocks used to form candidate particles, which ensures that energy deposits attributed to one particle are not used twice. The hadrons are then clustered together to form \textit{jets}, and these jets can additionally be identified as coming from tau leptons or b quarks\cite{Sirunyan_2017,PhysRevD.94.112002} (Figure \ref{fig:pf}). There is also Pileup Jet Identification (PU Jet ID)\cite{PFJetID}, a c-quark jet identification probability, as well as composite jet identification, e.g. a top-jet or a W-jet or Z-jet but these higher level objects are not relevant for this analysis.

\section{Jets}

During proton-proton collisions, the confined state of quarks and gluons is broken. Shortly after the collision, partons hadronize and a bunch of particles is generated by this process. These particles are usually collimated in a given direction due to the boosted nature of the parton, and thereby produce a jet or spray of particles around it.

\begin{figure}[h]
 	\centering
 	\includegraphics[width=0.75\textwidth]{figures/cmsjet.png}
 	\singlespace
 	\caption{Schematic view of a jet with tracks and calorimeter deposits at CMS. Reprinted from \cite{jetCMS}}
  	\label{fig:jets2} 	
 \end{figure}

\begin{figure}[h]
 	\centering
 	\includegraphics[width=0.65\textwidth]{figures/jme.png}
 	\singlespace
 	\caption{Particle composition for a jet. The energy fraction is relatively constant as a function of $p_{T}^{jet}$ and corresponds to roughly 65$\%$, 25$\%$, and 10$\%$ charged hadrons, photons, and neutral hadrons, respectively. Reprinted from \cite{Cacciari:2008gp}}
  	\label{fig:jme} 	
 \end{figure}

In practice, jets are the result of clustering groups of charged hadrons, photons, and neutral hadrons with the occasional muon and electron. The energy fraction in jets is divided amongst them with a breakdown of roughly 65$\%$, 25$\%$, and 10$\%$ respectively. This is illustrated in Figure \ref{fig:jme}. For this study, jets were reconstructed from PF candidates clusters using the anti-$k_{T}$ algorithm\cite{Cacciari:2008gp} as defined in the FASTJET package\cite{Cacciari:2011ma}.


 Jet clustering algorithms work by defining a distance parameter $d_{ij}$ between PF candidates $i$ and $j$ and the distance between such cluster and the beam $d_{iB}$. These are defined as

 \begin{align}
 \label{jetclust}
 d_{ij} &= min(k_{ti}^{2p},k_{tj}^{2p})\frac{\Delta_{ij}^{2}}{R^{2}}\\
 d_{iB} &= k_{ti}^{2p}
 \end{align}

where $\Delta_{ij}^{2} = (y_{i}-y_{j})^{2}+(\phi_{i}-\phi_{j})^{2}$, and $k_{ti}$, $y_{i}$, and $\phi_{i}$ are the transverse momentum, rapidity, and azimuth of particle $i$, respectively. R is a user-defined radius parameter, and $p$ is a measure of the relative power of energy vs geometric scales. Particularly, for the anti-$k_{T}$ algorithm, $p=-1$, and Equation \ref{jetclust} reduces to

\begin{equation}
d_{ij} = min(\frac{1}{p_{ti}^{2}},\frac{1}{p_{tj}^{2}})\frac{\Delta_{ij}^{2}}{R^{2}}
\end{equation}

The algorithm\cite{Cacciari:2011ma} loops over all PF candidate objects, calculating $d_{ij}$ for each pair of objects. Once it does this, it selects the two objects with the lowest value of $d_{ij}$ and combines them. Subsequently, it calculates the distance of any merged clusters to their nearest neighbors. This process is repeated until the smallest value of $d_{ij}$ satisfies the condition $d_{ij}>d_{iB}$.

As a result, the cutoff limit of $1/p_{T}^{2}$ defines a maximum size that the algorithm will look to cluster particles inside. The construction of $d_{ij}$ using the inverse $p_{T}^{2}$ has a result of producing values of $d_{ij}$ that are smaller for objects with a higher $p_{T}$, given equal separation. As a result, softer particles will tend to cluster to higher $p_{T}$ particles long before they would cluster amongst themselves. If no hard particles are present, the jet object will simply cluster soft $p_{T}$ particles in a circle in an $\eta-\phi$ space of radius R.

\begin{figure}[h]
 	\centering
 	\includegraphics[width=0.75\textwidth]{figures/antikt.png}
 	\singlespace
 	\caption{A sample parton-level event clustered with the anti-$k_{T}$ algorithm. Reprinted from \cite{Cacciari:2008gp}}.
  	\label{fig:antikt} 	
\end{figure}

The clustering of the anti-$k_{T}$ algorithm leads to jets with a large $p_{T}$ being reconstructed as perfect circles. Figure \ref{fig:antikt} shows a display of the anti-$k_{T}$ algorithm for a distance parameter R=1. Notice that the green jet around $y=2$ and $\phi=5$ has a circular shape, while the smaller jets loose some of their clusters to their higher momentum neighbors.

Finally, the anti-$k_{T}$ algorithm is both infrared and collinear safe. Infrared safety implies that the jet clustering algorithm is insensitive to the emission of soft, wide angle particles. Under this condition, two jets would not be merged due to one of them producing a soft-momentum particle between them. Collinear safety means that if there is a splitting which results in two parallel high-$p_{T}$ particles, a single jet is produced and the jet properties will not be different from a jet where this splitting did not occur. If the algorithm follows these two properties, it is referred to as being IRC safe.

The use of PF candidates, with their built-in tracking information for jet reconstruction provides a jet resolution of 15$\%$ at 10 GeV, 8$\%$ at 100 GeV, and 4$\%$ at 1 TeV\cite{CMS-PAS-PFT-09-001}. 

After the clustering procedure, the momentum and energy of the reconstructed jets still might not be the same as those from the initial parton. This could be due to out of cone showering, the presence of additional pileup energy produced during the same bunch crossing as the primary vertex, or detector effects. To correct for this, CMS adopted a factorized approach\cite{JINST2011}, where each level of correction targets a specific effect and each correction factor obtained is applied in order. The goal is to make sure each jet has a relative response

\begin{equation}
\mathcal{R}_{rel} = \frac{p_{T}^{reco}}{p_{T}^{ref}} 
\end{equation}

as close as possible to unity. Here $p_{T}^{reco}$ is the reconstructed jet $p_{T}$ and $p_{T}^{ref}$ is the true of reference $p_{T}$ of the jet at generator level. 

The process of correcting the jet 4-momentum by means of a scale or weight obtained from matching the reconstructed jet information to that of the reference jet in Monte Carlo is referred to as jet energy correction (JEC).

The first level of correction, commonly referred to as the L1FastJet\cite{Cacciari2007} corrections, starts by removing pileup or electronic noise energy that may have made it into the jet reconstruction. This multiplicative correction will only remove energy from within the jet and will take the form in Equation \ref{pileupeq}, where $\rho$ is the median energy density of the event, $A$ is the jet area, and $f$ is an estimate of the average amount of energy added to an event due to pileup (offset) inside the jet per unit of jet area \cite{JINST2011,Cacciari2007}.

\begin{equation}
\label{pileupeq}
p_{T}^{L1Corrected} = p_{T}^{uncorrected}\dot(1 - A\frac{f(\eta,\rho,A)}{p_{T}^{uncorrected}})
\end{equation}

While the L1 corrections attempt to remove pileup and electronic noise from jet energy measurements, the L2Relative and L3Absolute MC corrections attempt to correct the jet energy response so that it matches that of the particle level jet. The L2Relative correction compensates for the nonlinearity in the jet response as a function of $\eta$ while the L3Absolute correction does the same thing as a function of $p_{T}$. All three corrections are applied to both data and simulation. An additional level of correction, called L2L3Residual, is applied to data only, as a function of $\eta$, in order to correct for the difference in scale between the data and simulation.

A final level of modification to the reconstructed objects is an $\eta$ dependent smearing factor applied to the jet 4-momenta coming from the MC samples. The distribution of jet energies within the MC simulation tends to be more sharply peaked and less broad than the same distribution in data, resulting in a smaller jet energy resolution (JER) than we can realistically measure using the CMS detector. The deterministic "smearing" method recommended by CMS\cite{JetEnergyResolutionTwiki} matches the MC jet energy resolution to the one measured in data. 

The reconstructed jet $p_{T}$ is scaled by a correction factor $C_{JER}$ as determined in Equation \ref{eq:C_JER}, where $C_{\eta}$ is a correction factor derived as a function of $\eta$. The multiplicative JER correction factor is then used to modify the jet 4-momentum as in Equation \ref{eq:Jet_JER}.

\begin{equation}
\label{eq:C_JER}
C_{JER}=max\left(0.0,\frac{p_{T}^{GEN}}{p_{T}^{RECO}}+C_{\eta}\cdot\left(1-\frac{p_{T}^{GEN}}{p_{T}^{RECO}}\right)\right)
\end{equation}

\begin{equation}
\label{eq:Jet_JER}
\textbf{X}_{Jet}^{corrected}=C_{JER}{\cdot}\textbf{X}_{Jet}^{RECO}
\end{equation}

A set of quality cuts, collectively called PF jet identification, are applied to the resulting collection of jets to ensure that only real, hard scatter PF jets are used during the analysis\cite{CMS-AN-2010-003}. Several working point are defined at varying levels of efficiency and purity, but this analysis makes use of the tight criteria shown in Table \ref{tab:PFJetID}\cite{PFJetID}.

\begin{table}[htbp]
    \caption{Cut based PF jet identification requirements for the tight working point.}
    \centering
    \begin{tabular}{llll}
        \hline
        \multirow{2}{*}{Cut Variable}               & Cut Value \\\cline{2-2}
                                                    & Tight\\ 
        \hline 
        $\eta$& $|\eta|\leq$2.7 &2.7$<|\eta|\leq$ 3.0  & $|\eta|>$ 3.0\\
        \hline 
        Neutral Hadron Fraction  & <0.90  & <0.98 & -\\  
        Neutral EM Fraction      & <0.90  & > 0.01 & <0.90\\
        $n_{constituents}$       & >1     & -      & - \\
        Muon fraction            & <0.8   & -      & -\\
        Number of Neutral Particles & - & > 2      & >10\\
        \hline
        and for $|\eta| \leq$ 2.4 in addition apply\\
        \hline
        Charged Hadron Fraction & > 0 \\
        Charged Multiplicity    & > 0 \\
        Charged EM Fraction     & < 0.90\\
        \hline 
    \end{tabular}
    \label{tab:PFJetID}
\end{table}

All cuts on the jet energy fractions are made on the raw jets, before any energy correction is applied. In addition to the PF jet quality cuts, this analysis requires that all jets be within |$\eta$| < 2.6 and to have a $p_{T} > $ 30 GeV. 

\section{b-tagging}
Some jets are produced from a b-quark that after enters a bound state with another quark becoming part of a B meson that has a long lifetime which subsequently decays after it has traveled some distance. B-tagging is the identification of jets at some confidence level as having contained a B meson.

\begin{figure}[h]
 	\centering
 	\includegraphics[width=0.65\textwidth]{figures/Btag.png}
 	\singlespace
 	\caption{Diagram showing the common principle of identification of jets initiated by B hadron decays. Reprinted from \cite{wiki:btag}}
  	\label{fig:btagdia} 	
 \end{figure}

A variety of b-tagging algorithms has been developed by CMS to select b-quark jets\cite{BTV-12-001} based on variables such as the impact parameters of the charged-particle tracks, the properties of reconstructed decay vertices, and the presence or absence of a lepton, or combinations thereof. These algorithms heavily rely on machine learning tools like deep neural networks. In particular, CMS makes use of a new algorithm, DeepCSV\cite{Sirunyan_2018, PhysRevD.94.112002}, which uses a deep neural network trained by using about 50 million simulated jets. 

\begin{table}[htbp]
    \caption{Input variables used for the CSVv2 algorithm.}
    \centering
    \begin{tabular}{l}
        \hline
        %\multirow{2}{*}{Cut Variable}               & Cut Value \\\cline{2-2}
        Input variable                                            \\ 
        \hline 
        Secondary vertex 2D flight distance significance\\
        Number of secondary vertices\\
        Track $\eta_{rel}$\\
        Corrected secondary vertex mass\\
        Number of track from secondary vertex\\
        Secondary vertex energy ratio\\
        $\Delta R (Secondary vertex, jet)$ \\
        3D interaction point significance of the first four tracks\\
        Track $p_{T,rel}$\\
        $\Delta R (track, jet)$ \\
        Track $p_{T,rel}$ ratio\\
        Track distance\\
        Track decay length\\
        Summed tracks $E_{T}$ ratio\\
        $\Delta R$(summed tracks, jet)\\
        First track 2D interaction point significance above c threshold\\
        Number of selected tracks\\
        Jet $p_{T}$\\
        Jet $\eta$\\
        \hline 
    \end{tabular}
    \label{tab:csvv2var}
\end{table}

The DeepCVS algorithm uses the reconstructed tracks and secondary vertices found by using the \textit{inclusive vertex finding} (IVF) algorithm \cite{ADAM2007781}. The same input variables used for the CSV(Combined Secondary Vertex)v2 tagger (Table \ref{tab:csvv2var} from \cite{Sirunyan_2018}) are used, with the difference that the track-based variables use up to six tracks in the training of the DeepCSV. Jets are randomly selected in such a way that similar jet $p_{T}$ and $\eta$ distributions are obtained for all jet flavors. These distributions are also used as input variables in the training to take into account the correlation between the jet kinematics and the other variables. The distribution of all input variables is preprocessed to center the mean of each distribution around zero and to obtain a root-mean-square value of unity. All of the variables are presented to the multi-variable analysis (MVA) in the same way because of the preprocessing.

The training is performed using jets with $p_{T}$ between 20 GeV and 1 TeV, and within the tracker acceptance. The relative ratio of jets of each flavor is set to 2:1:4 for b:c:udsg jets. a mixture of $t\bar{t}$ and multi-jet events is used to reduce the possible dependency of the training on the heavy-flavor quark production process.

The training of the deep neural network is performed using the KERAS\cite{chollet2015keras} deep learning library, interfaced with the TENSORFLOW\cite{tensorflow2015-whitepaper} library that is used for low-level operations such as convolutions. The neural network uses four hidden layers that are fully connected, each with 100 nodes. For the nodes in the last layer, a normalized exponential function is used for the activation to be able to interpret the output value as a probability for a certain jet flavor category, $P(f)$. The output layer contains five nodes corresponding to five jet flavor categories used in the training. These categories are defined according to whether the jet contains exactly one b hadron, at least two b hadrons, exactly one c hadron and no b hadrons, at least two c hadrons and no b hadrons, or none of the aforementioned categories. Each of these categories is completely independent of the others, and the reasoning behind the chosen categorization has to do with the ability of identifying jets containing two b or c hadrons. 

The tagger can categorize individual jets in so-called "Tight" (DeepCSVT), "Medium"(DeepCSVM), and "Loose"(DeepCSVL) categories or working points. These working points correspond to 0.1, 1, and 10 $\%$ misidentification rates, respectively.

\begin{figure}[h]
 	\centering
 	\includegraphics[width=0.55\textwidth]{figures/effvspt_b_deep.png}
 	\singlespace
 	\caption{b-jet efficiency as a function of jet $p_{T}$ for the DeepCSV algorithm for different working points\cite{Sirunyan_2018}}
 	  	\label{fig:deepcsv}
 \end{figure}

Figure \ref{fig:deepcsv} shows the b-jet efficiency as a function of the jet $p_{T}$ for the DeepCSV algorithm at different working points. These efficiencies are obtained on simulated $t\bar{t}$ events using jets within tracker acceptance with $p_{T}$ > 20 GeV.



\section{Event Generation}
Searching for new physics can basically be reduced to a search for deviations from the SM. For this reason, extremely accurate signal modeling and SM background predictions are required. For a given analysis, these predictions take the form of samples of events representing various physics scenarios, as they would be seen in the CMS detector. Monte Carlo (MC) event generators are used to simulate proton-proton collisions resulting in a variety of final states. The passage of these final states through the CMS detector is then simulated, and the resulting detector level data is analyzed. The reconstruction algorithms described in the previous section are then run on the simulated data, allowing for a direct comparison with real data.

\subsection{Event Generators}
Particle physics event generators aim to give a complete description of particle collisions. This involves a combination of perturbative physics at large energy scales, and non-perturbative physics at small energy scales, which is described by phenomenological models.

In practice, event generators are software packages which take a specific initial state as input and simulate a selected subset of outcomes, from an interaction between the specified initial state particles. Bare partons produced in the hard scattering undergo gluon radiation or splitting, and subsequently undergo hadronization to form colorless hadrons. Unstable particles produced in the hard interaction are made to decay to stable particles according to their known, or imposed, branching fractions and lifetimes.

During the event generation process, strongly-interacting particles which take part in hard scattering interactions with large momentum transfer can be considered free due to asymptotic freedom. The large energy scale involved allows treatment of the interaction using perturbative methods. Under these assumptions, the proton-proton cross section at the LHC for a given $N$ particle final state is given by

\begin{equation}
\sigma_{N} = \sum_{a,b}\int_{0}^{1}dx_{1}\int_{0}^{1}dx_{2}f_{a}(x_{1},\mu^{2})f_{b}(x_{2},\mu^{2})\hat{\sigma}_{N}^{ab}
\end{equation}

\begin{figure}[h]
 	\centering
 	\includegraphics[width=0.55\textwidth]{figures/pdfs.png}
 	\singlespace
 	\caption{Parton distribution functions at the $\mu^{2}=10^{4}GeV^{2}$ mass scale. Here, the vertical axis is the number density of the parton. Effectively it's a probability density, but normalized to the expected number of a given parton in the proton. The width in each line is the uncertainty in the PDF for that particular parton\cite{Martin:2009iq}}
 	\label{fig:pdfs}
 \end{figure}

where the sum is over all parton species $a$ and $b$ within protons 1 and 2. $f_{i}(x_{j},\mu^{2})$ is the probability (calculated at renormalization scale $\mu^{2}$) of finding parton species $i$ carrying a momentum fraction $x_{j}$ of the parent proton $j$, and $\hat{\sigma}_{N}^{ab}$ is the partonic cross section for initial state $a+b$. The function $f_{i}(x_{j},\mu^{2})$ is referred to as a parton distribution function (PDF), and usually refers to the probability density for finding a particle with a certain longitudinal momentum fraction $x$ at a resolution scale $\mu^{2}$ (Figure \ref{fig:pdfs} is an example of such distribution at the $10^{4}$ GeV$^{2}$ mass scale).

 The main task involved in simulation of the hard interaction is the evaluation of the integral in Equation \ref{eq:event_generation_partonic_cross_section}. The partonic cross section is itself given by

\begin{equation}
\begin{split}\label{eq:event_generation_partonic_cross_section}
\hat{\sigma}_{N}^{ab}=\int_{cuts}d\hat{\sigma}_{N}^{ab}={}&\frac{\left(2\pi\right)^{4}S}{4\sqrt{\left(p_{1}{\cdot}p_{2}\right)^{2}-m_{1}^{2}m_{2}^{2}}}\times \\ &\int_{cuts} \left[\prod_{i=1}^{N}\frac{d^{3}q_{i}}{\left(2\pi\right)^{3}2E_{i}}\right]\delta^{4}\left(p_{1}+p_{2}-\sum_{i}^{N}q_{i}\right)|\mathcal{M}_{p_{1}p_{2}\rightarrow\{\vec{q}\}}^{ab}|^{2}
\end{split}
\end{equation}

where $p_{i}$ are the incoming particle four-momenta, $q_{i}(E_{i})$ are the outgoing particle four-momenta, $S$ is a product of factors $1/j!$ for each set of $j$ identical particles in the final state, and $\mathcal{M}_{p_{1}p_{2}\rightarrow\{\vec{q}\}}^{ab}$ is the parton level matrix element (ME) for the process. The event generator must build and evaluate all Feynman diagrams associated with the given process to determine the parton level ME, or these must be hard-coded by the package authors. 

The number of diagrams is directly proportional to the final state multiplicity and therefore becomes a complex problem very quickly. Next-to-leading-order (NLO) generators have recently been developed which include loop diagrams. This inclusion complicates the ME calculation enormously as divergences arise in real and virtual contributions which must cancel. 

Once the MEs have been evaluated, the evaluation of the multidimensional phase space integration required for the random sampling is performed using MC integration techniques\cite{Binder_1997}. 

The underlying event (UE) refers to all the semi-hard interactions that the spectator partons that did not take place in the initial hard interaction undergo with each other. Because these spectator interactions are typically soft, they are not calculable by perturbative methods and empirical models are used to describe them.

Bare partons may be produced as a result of the hard interaction. These strongly-interacting particles are perturbatively evolved from the scale of the hard interaction through successive branchings down to a lower energy scale at which they combine to form colorless hadrons, the hadronization scale. These successive branchings are the origin of hadronic jets, whereby individual quarks and gluons lead to a cascade of particles moving in the general direction of the original parton as they inherit its momentum. The probability for a quark or gluon to branch into two partons as it evolves from scale $t$ to $t'<t$ as well as the kinematics of such a branching can be calculated from first principles, accurate to fixed order in the strong coupling; the results of which are known as the DGLAP evolution equations\cite{Altarelli:1977zs}. Thus, partons are recursively evolved down to the hadronization scale through successive branchings. After showering of an $N-1$ particle final state, an additional hard parton can be radiated, thus producing overlap with an $N$ particle interaction hard state. The colored proton remnants which did not take part in the hard interaction can produce showers as well. 

At the hadronization scale, the showering ceases and the colored partons group to form colorless hadrons. This regime is not amenable to perturbative calculations, and no first-principle theory is viable. Various phenomenological methods have been developed to model hadronization, including the \textit{Lund-string-model}\cite{ANDERSSON198331}. In the Lund model, quarks $q$ and anti-quarks $\bar{q}$ are the end points of Lund strings, in a similar way to that of the positive and negative charges of electric dipoles. Unlike the electrodynamic field, the chromodynamic field is self-interacting, and the field lines can be thought of as forming a bundle or a strong. Gluons $g$ are excitations or force carriers on these string lines. Once strings are formed, they dissolve or fragment into hadrons based on a probability distribution derived from data. The number, types, and kinematics of particles produced from the string fragmentation depend upon the number, types and kinematics of partons that form them. 

In this analysis the generation of physical events proceeds in three steps, the first one being the ME calculation using MADGRAPH. Then PYTHIA\cite{Sjostrand:2014zea} is used to simulate parton shower and hadronization.

PYTHIA is a general purpose, tree level partonic matrix element generator capable of performing parton showering, hadronization, and UE simulation. A variety of $2\rightarrow1,2,3$ processes are included. Full spin correlations are included in the decays of unstable resonances. Shower evolution proceeds in terms of decreasing time-like virtuality, and imposes angular ordering by veto. Shower evolution is accurate to the LL level. The Lund string model is used for hadronization. UE interactions are described perturbatively as multiple nearly-independent $2\rightarrow 2$ scatterings. For this study, PYTHIA 8 is used to simulate signal samples with $xqcut=30$ and $qcut=60$.

MADGRAPH\cite{Alwall:1647049} is used to simulate hard parton emission (i.e. ISR and FSR) and is interfaced with PYTHIA for showering and collinear radiation.

\subsection{Detector Simulation}

The next step in the simulation of events chain is the simulation of how particles will interact with the detector and its constituent materials and how the readout electronics will behave. To simulate the response of the CMS detector, the generators are interfaced with a sophisticated detector simulation based on the GEANT4\cite{geant4sim} software package, which takes into account the exact detector geometry as well as all materials used.

The alignment, calibration, and other conditions which may change over time are periodically checked and stored in a database. These conditions are used for both offline simulation and reconstruction as well as for online activities. A snapshot of the conditions at some point in time is called a global tag. For reference, this analysis uses the $80X\_dataRun2\_2016LegacyRepro\_v4$ and $80X\_mcRun2\_asymptotic\_2016\_TrancheIV\_v8$ global tags for data and simulation, respectively. 

The final state particles from the event generator are sent to the detector simulation, which tracks the particles as they move through the detector depositing energy into what are called simulated hits. While the models of electromagnetic interactions are extremely precise, the hadronic interactions have a greater uncertainty associated with them. The simulation goes through the data acquisition process, simulating the responses of the avalanche photodiodes and readout electronics. The resulting information is then analyzed by the same reconstruction process that the real data undergoes and is stored using the ROOT software library. 

